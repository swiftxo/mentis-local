{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: mps\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n"
     ]
    }
   ],
   "source": [
    "from archivum.db import Archivum\n",
    "import archivum.config as config\n",
    "\n",
    "db = Archivum(\n",
    "    storage_path=config.VECTOR_DB_PATH,\n",
    "    collection_name=config.COLLECTION_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soho/Archivum Mentis/code/mentis-local/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer|Load] Loading tokenizer from: assets/tokenizer.json\n",
      "[Tokenizer|Load] Tokenizer loaded. Padding to 512 tokens.\n",
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: mps\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\n",
      "[Parser|Dispatch] File extension detected: .pdf\n",
      "[Parser|PDF] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\n",
      "[Parser|PDF] Parsed 66 pages with text.\n",
      "[Chunking|Sentence] Created 13 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 13 | Avg tokens/chunk: 39.4 | Time: 0.0171s\n",
      "[Chunking|Sentence] Created 21 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 21 | Avg tokens/chunk: 24.4 | Time: 0.0035s\n",
      "[Chunking|Sentence] Created 2 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 2 | Avg tokens/chunk: 256.0 | Time: 0.0005s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0039s\n",
      "[Chunking|Sentence] Created 28 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 28 | Avg tokens/chunk: 18.3 | Time: 0.0045s\n",
      "[Chunking|Sentence] Created 30 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 30 | Avg tokens/chunk: 17.1 | Time: 0.0043s\n",
      "[Chunking|Sentence] Created 12 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 12 | Avg tokens/chunk: 42.7 | Time: 0.0018s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0038s\n",
      "[Chunking|Sentence] Created 19 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 19 | Avg tokens/chunk: 26.9 | Time: 0.0028s\n",
      "[Chunking|Sentence] Created 3 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 3 | Avg tokens/chunk: 170.7 | Time: 0.0006s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.0029s\n",
      "[Chunking|Sentence] Created 2 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 2 | Avg tokens/chunk: 256.0 | Time: 0.0004s\n",
      "[Chunking|Sentence] Created 9 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 9 | Avg tokens/chunk: 56.9 | Time: 0.0017s\n",
      "[Chunking|Sentence] Created 13 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 13 | Avg tokens/chunk: 39.4 | Time: 0.0021s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0032s\n",
      "[Chunking|Sentence] Created 17 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 17 | Avg tokens/chunk: 30.1 | Time: 0.0028s\n",
      "[Chunking|Sentence] Created 13 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 13 | Avg tokens/chunk: 39.4 | Time: 0.002s\n",
      "[Chunking|Sentence] Created 17 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 17 | Avg tokens/chunk: 30.1 | Time: 0.0022s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0042s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.0032s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.0028s\n",
      "[Chunking|Sentence] Created 2 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 2 | Avg tokens/chunk: 256.0 | Time: 0.0004s\n",
      "[Chunking|Sentence] Created 21 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 21 | Avg tokens/chunk: 24.4 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 16 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 16 | Avg tokens/chunk: 32.0 | Time: 0.0023s\n",
      "[Chunking|Sentence] Created 15 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 15 | Avg tokens/chunk: 34.1 | Time: 0.0021s\n",
      "[Chunking|Sentence] Created 30 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 30 | Avg tokens/chunk: 17.1 | Time: 0.0039s\n",
      "[Chunking|Sentence] Created 25 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 25 | Avg tokens/chunk: 20.5 | Time: 0.0031s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.0021s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.0023s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 25 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 25 | Avg tokens/chunk: 20.5 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0039s\n",
      "[Chunking|Sentence] Created 17 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 17 | Avg tokens/chunk: 30.1 | Time: 0.0022s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.0036s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.002s\n",
      "[Chunking|Sentence] Created 25 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 25 | Avg tokens/chunk: 20.5 | Time: 0.0032s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.003s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0036s\n",
      "[Chunking|Sentence] Created 19 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 19 | Avg tokens/chunk: 26.9 | Time: 0.0032s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.0034s\n",
      "[Chunking|Sentence] Created 12 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 12 | Avg tokens/chunk: 42.7 | Time: 0.0027s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.0029s\n",
      "[Chunking|Sentence] Created 18 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 18 | Avg tokens/chunk: 28.4 | Time: 0.0022s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0034s\n",
      "[Chunking|Sentence] Created 9 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 9 | Avg tokens/chunk: 56.9 | Time: 0.0014s\n",
      "[Chunking|Sentence] Created 15 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 15 | Avg tokens/chunk: 34.1 | Time: 0.0023s\n",
      "[Chunking|Sentence] Created 18 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 18 | Avg tokens/chunk: 28.4 | Time: 0.0029s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0036s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.0019s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.003s\n",
      "[Chunking|Sentence] Created 8 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 8 | Avg tokens/chunk: 64.0 | Time: 0.0014s\n",
      "[Chunking|Sentence] Created 27 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 27 | Avg tokens/chunk: 19.0 | Time: 0.0035s\n",
      "[Chunking|Sentence] Created 22 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 22 | Avg tokens/chunk: 23.3 | Time: 0.0029s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0038s\n",
      "[Chunking|Sentence] Created 9 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 9 | Avg tokens/chunk: 56.9 | Time: 0.0017s\n",
      "[Chunking|Sentence] Created 22 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 22 | Avg tokens/chunk: 23.3 | Time: 0.0035s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.003s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.0042s\n",
      "[Chunking|Sentence] Created 4 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 4 | Avg tokens/chunk: 128.0 | Time: 0.0012s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0037s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.0053s\n",
      "[Chunking|Sentence] Created 16 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 16 | Avg tokens/chunk: 32.0 | Time: 0.0027s\n",
      "[Chunking|Sentence] Created 18 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 18 | Avg tokens/chunk: 28.4 | Time: 0.0029s\n",
      "[Chunking|Sentence] Created 26 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 26 | Avg tokens/chunk: 19.7 | Time: 0.0037s\n",
      "[Chunking|Sentence] Created 34 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 34 | Avg tokens/chunk: 15.1 | Time: 0.0038s\n",
      "[Archivum|Add] Embedding 1240 documents...\n",
      "[Embedder|Embed] Embedding 1240 texts... Batch size: 32 | Normalize: True | Tensor Output: False\n",
      "[Archivum|Add] Added 1240 documents to collection 'archivum_fragments'.\n",
      "[Ingest] Ingested 1240 chunks from 1 new files into collection 'archivum_fragments'.\n"
     ]
    }
   ],
   "source": [
    "from archivum.ingest import ingest\n",
    "ingest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: mps\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n",
      "[Query|Retrieve+Meta] Retrieving top 5 chunks + metadata for query: 'what is the internet?'\n",
      "[Archivum|Query] Querying for: 'what is the internet?' | Top 5 results\n",
      "[Embedder|Embed] Embedding 1 texts... Batch size: 32 | Normalize: True | Tensor Output: False\n",
      "[Archivum|Query] Retrieved 5 chunks.\n",
      "[Query|Retrieve+Meta] Retrieved 5 chunks with metadata.\n",
      "\n",
      "=== Retrieved Chunks ===\n",
      "Chunk 1:\n",
      "Text: 1.1.1 A Nuts-and-Bolts Description\n",
      "The Internet is a computer network that interconnects billions of computing devices \n",
      "throughout the world.\n",
      "Metadata: {\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"page_number\": 2,\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\"\n",
      "}\n",
      "Distance: 0.25795382261276245\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Text: Network Layer\n",
      "The Internet’s network layer is responsible for moving network-layer packets known \n",
      "as datagrams from one host to another.\n",
      "Metadata: {\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\",\n",
      "  \"page_number\": 51,\n",
      "  \"file_type\": \"pdf\"\n",
      "}\n",
      "Distance: 0.28405648469924927\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "Text: In summary, today’s Internet—a network of networks—is complex, consisting \n",
      "of a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs.\n",
      "Metadata: {\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\",\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"page_number\": 34\n",
      "}\n",
      "Distance: 0.28843069076538086\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "Text: Link Layer\n",
      "The Internet’s network layer routes a datagram through a series of routers between \n",
      "the source and destination.\n",
      "Metadata: {\n",
      "  \"page_number\": 51,\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\",\n",
      "  \"file_type\": \"pdf\"\n",
      "}\n",
      "Distance: 0.2886946201324463\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "Text: Furthermore, nontraditional \n",
      "Internet “things” such as TVs, gaming consoles, thermostats, home security systems, \n",
      "home appliances, watches, eye glasses, cars, traffic control systems, and more are \n",
      "being connected to the Internet.\n",
      "Metadata: {\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/chapter1.pdf\",\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"page_number\": 2\n",
      "}\n",
      "Distance: 0.2922152876853943\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from archivum.query import retrieve_relevant_chunks, retrieve_chunks_with_metadata\n",
    "query_text = input(\"Enter your query: \")\n",
    "chunks = retrieve_chunks_with_metadata(query_text, n_results=5)\n",
    "print(\"\\n=== Retrieved Chunks ===\")\n",
    "for idx, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {idx}:\")\n",
    "    print(f\"Text: {chunk['text']}\")\n",
    "    print(f\"Metadata: {json.dumps(chunk['metadata'], indent=2)}\")\n",
    "    print(f\"Distance: {chunk['distance']}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
