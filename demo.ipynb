{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: cpu\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n"
     ]
    }
   ],
   "source": [
    "from archivum.db import Archivum\n",
    "import archivum.config as config\n",
    "\n",
    "db = Archivum(\n",
    "    storage_path=config.VECTOR_DB_PATH,\n",
    "    collection_name=config.COLLECTION_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tokenizer|Load] Loading tokenizer from: assets/tokenizer.json\n",
      "[Tokenizer|Load] Tokenizer loaded. Padding to 512 tokens.\n",
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: cpu\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/treaty_of_versaille_pdf.pdf\n",
      "[Parser|Dispatch] File extension detected: .pdf\n",
      "[Parser|PDF] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/treaty_of_versaille_pdf.pdf\n",
      "[Parser|PDF] Parsed 10 pages with text.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/cactus.md\n",
      "[Parser|Dispatch] File extension detected: .md\n",
      "[Parser|MD] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/cactus.md\n",
      "[Parser|MD] Parsed markdown file into one chunk.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/network_congestion.md\n",
      "[Parser|Dispatch] File extension detected: .md\n",
      "[Parser|MD] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/network_congestion.md\n",
      "[Parser|MD] Parsed markdown file into one chunk.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/military_logistics.md\n",
      "[Parser|Dispatch] File extension detected: .md\n",
      "[Parser|MD] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/military_logistics.md\n",
      "[Parser|MD] Parsed markdown file into one chunk.\n",
      "[Ingest] New file detected: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\n",
      "[Parser|Dispatch] File extension detected: .pdf\n",
      "[Parser|PDF] Parsing file: /Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\n",
      "[Parser|PDF] Parsed 15 pages with text.\n",
      "[Chunking|Sentence] Created 4 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 4 | Avg tokens/chunk: 128.0 | Time: 0.0173s\n",
      "[Chunking|Sentence] Created 11 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 11 | Avg tokens/chunk: 46.5 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 19 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 19 | Avg tokens/chunk: 26.9 | Time: 0.003s\n",
      "[Chunking|Sentence] Created 21 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 21 | Avg tokens/chunk: 24.4 | Time: 0.0036s\n",
      "[Chunking|Sentence] Created 19 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 19 | Avg tokens/chunk: 26.9 | Time: 0.003s\n",
      "[Chunking|Sentence] Created 15 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 15 | Avg tokens/chunk: 34.1 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 16 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 16 | Avg tokens/chunk: 32.0 | Time: 0.0039s\n",
      "[Chunking|Sentence] Created 14 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 14 | Avg tokens/chunk: 36.6 | Time: 0.0034s\n",
      "[Chunking|Sentence] Created 17 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 17 | Avg tokens/chunk: 30.1 | Time: 0.0027s\n",
      "[Chunking|Sentence] Created 5 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 5 | Avg tokens/chunk: 102.4 | Time: 0.0007s\n",
      "[Chunking|Sentence] Created 641 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 641 | Avg tokens/chunk: 0.8 | Time: 0.1198s\n",
      "[Chunking|Sentence] Created 165 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 165 | Avg tokens/chunk: 3.1 | Time: 0.0296s\n",
      "[Chunking|Sentence] Created 1475 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 1475 | Avg tokens/chunk: 0.3 | Time: 0.1753s\n",
      "[Chunking|Sentence] Created 20 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 20 | Avg tokens/chunk: 25.6 | Time: 0.0039s\n",
      "[Chunking|Sentence] Created 24 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 24 | Avg tokens/chunk: 21.3 | Time: 0.0061s\n",
      "[Chunking|Sentence] Created 15 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 15 | Avg tokens/chunk: 34.1 | Time: 0.0025s\n",
      "[Chunking|Sentence] Created 19 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 19 | Avg tokens/chunk: 26.9 | Time: 0.0033s\n",
      "[Chunking|Sentence] Created 28 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 28 | Avg tokens/chunk: 18.3 | Time: 0.0046s\n",
      "[Chunking|Sentence] Created 23 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 23 | Avg tokens/chunk: 22.3 | Time: 0.005s\n",
      "[Chunking|Sentence] Created 26 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 26 | Avg tokens/chunk: 19.7 | Time: 0.0045s\n",
      "[Chunking|Sentence] Created 21 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 21 | Avg tokens/chunk: 24.4 | Time: 0.004s\n",
      "[Chunking|Sentence] Created 21 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 21 | Avg tokens/chunk: 24.4 | Time: 0.0044s\n",
      "[Chunking|Sentence] Created 35 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 35 | Avg tokens/chunk: 14.6 | Time: 0.0049s\n",
      "[Chunking|Sentence] Created 62 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 62 | Avg tokens/chunk: 8.3 | Time: 0.0069s\n",
      "[Chunking|Sentence] Created 53 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 53 | Avg tokens/chunk: 9.7 | Time: 0.0068s\n",
      "[Chunking|Sentence] Created 8 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 8 | Avg tokens/chunk: 64.0 | Time: 0.001s\n",
      "[Chunking|Sentence] Created 9 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 9 | Avg tokens/chunk: 56.9 | Time: 0.001s\n",
      "[Chunking|Sentence] Created 8 sentence-based chunks.\n",
      "[Chunking|Summary] Strategy: sentence | Tokens: 512 | Chunks: 8 | Avg tokens/chunk: 64.0 | Time: 0.0009s\n",
      "[Archivum|Add] Embedding 2794 documents...\n",
      "[Embedder|Embed] Embedding 2794 texts... Batch size: 32 | Normalize: True | Tensor Output: False\n",
      "[Archivum|Add] Added 2794 documents to collection 'archivum_fragments'.\n",
      "[Ingest] Ingested 2794 chunks from 5 new files into collection 'archivum_fragments'.\n"
     ]
    }
   ],
   "source": [
    "from archivum.ingest import ingest\n",
    "ingest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Embedder|Load] Loading embedder model: intfloat/multilingual-e5-large-instruct on device: cpu\n",
      "[Embedder|Load] Model loaded successfully.\n",
      "[Archivum|Init] Connected to collection 'archivum_fragments' at '/Users/soho/Archivum Mentis/code/mentis-local/archivum/vectordb'.\n",
      "[Query|Retrieve+Meta] Retrieving top 5 chunks + metadata for query: 'What are the tradeoffs between learned positional embeddings and sinusoidal ones?'\n",
      "[Archivum|Query] Querying for: 'What are the tradeoffs between learned positional embeddings and sinusoidal ones?' | Top 5 results\n",
      "[Embedder|Embed] Embedding 1 texts... Batch size: 32 | Normalize: True | Tensor Output: False\n",
      "[Archivum|Query] Retrieved 5 chunks.\n",
      "[Query|Retrieve+Meta] Retrieved 5 chunks with metadata.\n",
      "\n",
      "=== Retrieved Chunks ===\n",
      "Chunk 1:\n",
      "Text: In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
      "results to the base model.\n",
      "Metadata: {\n",
      "  \"page_number\": 9,\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\",\n",
      "  \"file_type\": \"pdf\"\n",
      "}\n",
      "Distance: 0.2168596237897873\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Text: We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)).\n",
      "Metadata: {\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\",\n",
      "  \"page_number\": 6,\n",
      "  \"file_type\": \"pdf\"\n",
      "}\n",
      "Distance: 0.2638648748397827\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "Text: The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed.\n",
      "Metadata: {\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\",\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"page_number\": 6\n",
      "}\n",
      "Distance: 0.29013603925704956\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "Text: We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "Metadata: {\n",
      "  \"page_number\": 6,\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\",\n",
      "  \"file_type\": \"pdf\"\n",
      "}\n",
      "Distance: 0.2953695058822632\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "Text: In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.\n",
      "Metadata: {\n",
      "  \"page_number\": 2,\n",
      "  \"file_type\": \"pdf\",\n",
      "  \"file_path\": \"/Users/soho/Archivum Mentis/code/mentis-local/input/sample/attention_is_all_you_need.pdf\"\n",
      "}\n",
      "Distance: 0.2970142960548401\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from archivum.query import retrieve_relevant_chunks, retrieve_chunks_with_metadata\n",
    "query_text = input(\"Enter your query: \")\n",
    "chunks = retrieve_chunks_with_metadata(query_text, n_results=5)\n",
    "print(\"\\n=== Retrieved Chunks ===\")\n",
    "for idx, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {idx}:\")\n",
    "    print(f\"Text: {chunk['text']}\")\n",
    "    print(f\"Metadata: {json.dumps(chunk['metadata'], indent=2)}\")\n",
    "    print(f\"Distance: {chunk['distance']}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
