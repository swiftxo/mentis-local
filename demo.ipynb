{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Step 1: Parse PDF into a Document object\n",
    "\n",
    "This step loads a `.pdf` file and converts it into a `Document`, which includes both the full text and metadata (like page numbers).  \n",
    "- The `Document` also splits the text into internal chunks, typically one per page or section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document with 422 chunks>\n"
     ]
    }
   ],
   "source": [
    "from archivum.parser import parse_file\n",
    "file_path = \"input/canadian_federal_budget_2024.pdf\"\n",
    "document = parse_file(file_path)\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Step 2: Tokenize Chunks (Basic)\n",
    "\n",
    "This loop tokenizes each chunk of text using the configured tokenizer.  \n",
    "It prints:\n",
    "- `Tokens`: number of tokens in the chunk\n",
    "- `Mask Sum`: number of \"active\" tokens after padding (should match `Tokens` if no padding)\n",
    "\n",
    "This is helpful to check chunk size relative to your model's token limit (512 for e5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 512, Mask Sum: 427\n"
     ]
    }
   ],
   "source": [
    "from archivum.tokenizer_utils import load_tokenizer, tokenize, tokenize_raw\n",
    "tokenizer = load_tokenizer()\n",
    "\n",
    "for chunk in document.get_chunks():\n",
    "    input_ids, attention_mask = tokenize(chunk.text, tokenizer)\n",
    "    print(f\"Tokens: {len(input_ids)}, Mask Sum: {sum(attention_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 2b: Inspect Raw Tokenization\n",
    "\n",
    "This shows raw token details:\n",
    "- `tokens`: list of decoded token strings\n",
    "- `ids`: actual token IDs\n",
    "- `offsets`: character ranges of each token in the original string\n",
    "\n",
    "This is for debugging and analyzing the differnece between original text and token chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 tokens: ['<s>', '‚ñÅ*', 'Archiv', 'um', '‚ñÅMen', 'tis', '*', '‚ñÅdirectly', '‚ñÅtrans', 'late', 's', '‚ñÅto', '‚ñÅ\"', 'Archiv', 'e', '‚ñÅof', '‚ñÅthe', '‚ñÅMind', '\".', '‚ñÅ**', 'üìú', '‚ñÅE', 'tym', 'ology', '‚ñÅof', '‚ñÅ_', 'Archiv', 'um', '_', '**', '‚ñÅ>', '‚ñÅThe', '‚ñÅEnglish', '‚ñÅword', '‚ñÅ_', 'archive', '_', '‚ñÅis', '‚ñÅderi', 'ved', '‚ñÅfrom', '‚ñÅthe', '‚ñÅFrench', '‚ñÅ_', 'archive', 's', '_', '‚ñÅ(', 'pl', 'ural', '),', '‚ñÅand', '‚ñÅin', '‚ñÅturn', '‚ñÅfrom', '‚ñÅLatin', '‚ñÅ_', 'arch', 'ƒ´', 'um', '_', '‚ñÅor', '‚ñÅ_', 'arch', 'ƒ´vu', 'm', '_', ',', '‚ñÅthe', '‚ñÅroman', 'ized', '‚ñÅform', '‚ñÅof', '‚ñÅthe', '‚ñÅGreek', '‚ñÅ', '·ºÄ', 'œÅ', 'œáŒµ', '·øñ', 'ŒøŒΩ', '‚ñÅ(', '_', 'ar', 'khe', 'ion', '_', ').', '‚ñÅThe', '‚ñÅGreek', '‚ñÅterm', '‚ñÅoriginal', 'ly', '‚ñÅrefer', 'red', '‚ñÅto', '‚ñÅthe', '‚ñÅhome', '‚ñÅor', '‚ñÅd']\n",
      "Token count: 512\n",
      "Offsets: [(0, 0), (0, 1), (1, 7), (7, 9), (10, 13), (13, 16), (16, 17), (18, 26), (27, 32), (32, 36)]\n"
     ]
    }
   ],
   "source": [
    "for chunk in document.get_chunks():\n",
    "    enc = tokenize_raw(chunk.text, tokenizer)\n",
    "    print(f\"First 100 tokens: {enc.tokens[:100]}\")\n",
    "    print(f\"Token count: {len(enc.ids)}\")\n",
    "    print(f\"Offsets: {enc.offsets[:10]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Step 3: Sentence-Aware Chunking\n",
    "\n",
    "This uses full sentence boundaries to chunk text under a token limit (e.g., 512 tokens).  \n",
    "Sentences are added one-by-one until the chunk hits the limit. This ensures:\n",
    "- No mid-sentence cuts\n",
    "- Clean semantic groupings\n",
    "\n",
    "You can inspect how many chunks were made per document section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Sentence-aware Chunking ===\n",
      "\n",
      "[Chunking] Strategy: sentence | Tokens: 512 | Chunks: 10 | Avg tokens/chunk: 51.2 | Time: 0.0138s\n",
      "Document Chunk 1 ‚Üí 10 sentence chunks\n",
      "\n",
      "  [1] *Archivum Mentis* directly translates to \"Archive of the Mind\"....\n",
      "\n",
      "  [2] **üìú Etymology of _Archivum_**\n",
      "\n",
      "> The English word _archive_ is derived from the French _archives_ (plural), and in turn from Latin _archƒ´um_ or _archƒ´vum_, the romanized form of the Greek ·ºÄœÅœáŒµ·øñŒøŒΩ (_ar...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/soho/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from archivum.chunker import chunk_text, debug_token_chunk\n",
    "print(\"\\n=== Running Sentence-aware Chunking ===\\n\")\n",
    "for i, chunk in enumerate(document.get_chunks()):\n",
    "    sentence_chunks = chunk_text(chunk.text, tokenizer, strategy=\"sentence\", log=True)\n",
    "    print(f\"Document Chunk {i+1} ‚Üí {len(sentence_chunks)} sentence chunks\\n\")\n",
    "    for j, c in enumerate(sentence_chunks[:2]):\n",
    "        print(f\"  [{j+1}] {c[:200]}...\\n\")\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™ü Step 3b: Sliding Window Chunking\n",
    "\n",
    "This cuts text into fixed-length token windows with some overlap (`stride`).  \n",
    "Useful for models that benefit from continuous context.\n",
    "\n",
    "- `window=150`: size of each chunk in tokens\n",
    "- `stride=30`: overlap between chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Sliding Window Chunking ===\n",
      "\n",
      "[Chunking] Strategy: sliding | Tokens: 512 | Chunks: 17 | Avg tokens/chunk: 30.1 | Time: 0.0029s\n",
      "Document Chunk 1 ‚Üí 17 sliding chunks\n",
      "\n",
      "  [1] *Archivum Mentis* directly translates to \"Archive of the Mind\". **üìú Etymology of _Archivum_** > The English word _archive_ is derived from the French _archives_ (plural), and in turn from Latin _archƒ´...\n",
      "\n",
      "  [2] > The English word _archive_ is derived from the French _archives_ (plural), and in turn from Latin _archƒ´um_ or _archƒ´vum_, the romanized form of the Greek ·ºÄœÅœáŒµ·øñŒøŒΩ (_arkheion_). The Greek term origin...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Running Sliding Window Chunking ===\\n\")\n",
    "for i, chunk in enumerate(document.get_chunks()):\n",
    "    sliding_chunks = chunk_text(chunk.text, tokenizer, strategy=\"sliding\", window=150, stride=30, log=True)\n",
    "    print(f\"Document Chunk {i+1} ‚Üí {len(sliding_chunks)} sliding chunks\\n\")\n",
    "    for j, c in enumerate(sliding_chunks[:2]):\n",
    "        print(f\"  [{j+1}] {c[:200]}...\\n\")\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Step 3c: Token Slice Debugging\n",
    "\n",
    "Visual debug tool to print the tokenized form of a slice of text.  \n",
    "Here it prints tokens 0‚Äì20 from the first chunk, showing what actual tokens get passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Debug: Manual Token Slice ===\n",
      "\n",
      "Tokens 0:20 ‚Üí [0, 661, 219548, 316, 1111, 1814, 1639, 105237, 3900, 19309, 7, 47, 44, 219548, 13, 111, 70, 29616, 740, 16459]\n",
      "Decoded Text ‚Üí *Archivum Mentis* directly translates to \"Archive of the Mind\". **\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Debug: Manual Token Slice ===\\n\")\n",
    "sample_text = document.get_chunks()[0].text\n",
    "debug_token_chunk(sample_text, tokenizer, 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Step 4: Generate Embeddings\n",
    "\n",
    "We collect all the sentence-based chunks across the whole document.These will be passed into the embedding model.Each chunk is converted into a vector using `e5-large-instruct`.  \n",
    "The result is a tensor of shape `(num_chunks, dim)`, where each row is a semantic fingerprint of the chunk.\n",
    "\n",
    "Printed outputs confirm:\n",
    "- Number of chunks embedded\n",
    "- First chunk preview\n",
    "- First 5 dimensions of its vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 10 chunks ‚Üí shape: torch.Size([10, 1024])\n",
      "Example chunk: *Archivum Mentis* directly translates to \"Archive of the Mind\"....\n",
      "Embedding (first 5 dims): tensor([ 0.0320, -0.0064, -0.0170, -0.0274,  0.0263], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "from archivum.embedder import load_embedder, embed_texts, get_detailed_instruct\n",
    "import torch\n",
    "all_chunks = []\n",
    "for chunk in document.get_chunks():\n",
    "    sentence_chunks = chunk_text(chunk.text, tokenizer, strategy=\"sentence\")\n",
    "    all_chunks.extend(sentence_chunks)\n",
    "\n",
    "model = load_embedder()\n",
    "chunk_embeddings = embed_texts(all_chunks, model)\n",
    "\n",
    "print(f\"Embedded {len(all_chunks)} chunks ‚Üí shape: {chunk_embeddings.shape}\")\n",
    "print(f\"Example chunk: {all_chunks[0][:100]}...\")\n",
    "print(f\"Embedding (first 5 dims): {chunk_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Step 5: Encode Query\n",
    "\n",
    "User provides a natural-language question.  \n",
    "It's wrapped with an instruction (e.g., `\"Retrieve relevant document information\"`) to guide the embedding model to treat it as a search query.\n",
    "\n",
    "Then it‚Äôs embedded into a vector for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"\\nüîç Your query: \").strip()\n",
    "detailed_query = get_detailed_instruct(\"Retrieve relevant document information\", query)\n",
    "query_embedding = embed_texts([detailed_query], model)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 6: Vector Search + Top Matches\n",
    "\n",
    "We compare the query vector with each chunk using dot product (cosine similarity if vectors are normalized).  \n",
    "Top-k most similar chunks are retrieved based on similarity score.\n",
    "The top results are printed with their similarity score and the first ~200 characters.  \n",
    "This simulates the ‚Äúretrieval‚Äù step in RAG ‚Äî you're seeing which document pieces best match your query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Top 3 Results:\n",
      "\n",
      "[92.23] **üìú Etymology of _Archivum_**\n",
      "\n",
      "> The English word _archive_ is derived from the French _archives_ (plural), and in turn from Latin _archƒ´um_ or _archƒ´vum_, the romanized form of the Greek ·ºÄœÅœáŒµ·øñŒøŒΩ (_ar...\n",
      "\n",
      "[90.93] > ‚Äî [_Wikipedia: Archive_](https://en.wikipedia.org/wiki/Archive)\n",
      "\n",
      "I first chose the word Archivum because I like latin terminology, however upon further digging and investigating the origin of this w...\n",
      "\n",
      "[86.63] *Archivum Mentis* directly translates to \"Archive of the Mind\"....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = (query_embedding @ chunk_embeddings.T) * 100  # shape: (num_chunks,)\n",
    "top_k = 3\n",
    "top_indices = torch.topk(scores, k=top_k).indices.tolist()\n",
    "\n",
    "print(f\"\\nüß† Top {top_k} Results:\\n\")\n",
    "for idx in top_indices:\n",
    "    print(f\"[{scores[idx]:.2f}] {all_chunks[idx][:200]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
